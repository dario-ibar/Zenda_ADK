{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ac7c28-bc97-449f-8f7f-71fbcc30111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La librería google-genai y el módulo de caching de Vertex AI se importaron correctamente. ¡Listo para usar!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.genai\n",
    "    import vertexai.preview.caching\n",
    "    print(\"La librería google-genai y el módulo de caching de Vertex AI se importaron correctamente. ¡Listo para usar!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error al importar la librería: {e}\")\n",
    "    print(\"Parece que la librería google-genai no está instalada o no es accesible en este entorno.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3e42e3-5002-4460-b7a5-c03ac620689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI...\n",
      "\n",
      "--- Error al crear el Context Cache ---\n",
      "Mensaje de error: 404 Publisher Model `projects/378891832439/locations/us-central1/publishers/google/models/gemini-1.5-pro-001` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "Posibles causas:\n",
      "- Permisos insuficientes para crear recursos de caching en tu proyecto.\n",
      "- Error en el nombre del modelo especificado.\n",
      "- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\n",
      "- El contenido es significativamente menor al tamaño mínimo para caching explícito (~4k tokens), aunque debería permitir crear el recurso igual.\n",
      "Si estás en Workbench, verifica que tienes permisos para Vertex AI.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 1.5 que quieres usar para el caché\n",
    "# Usaremos este por ahora, es compatible con 2.5 para caching y caching explícito\n",
    "MODEL_NAME = \"gemini-1.5-pro-001\"\n",
    "\n",
    "\n",
    "# Contenido de ejemplo para cachear\n",
    "# Aquí podrías poner después una pauta, una definición, etc.\n",
    "# Recuerda que para que el caching explícito sea efectivo,\n",
    "# el contenido debe tener al menos ~4096 tokens.\n",
    "# Este es solo un ejemplo para probar la creación del recurso.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Las pautas generales de Zenda son protocolos de acompañamiento y reflexión.\n",
    "Están diseñadas para guiar la conversación del agente Zenda con el cliente.\n",
    "Incluyen acciones, formas de actuar (cómo) y objetivos (para qué).\n",
    "Son clave para asegurar la calidad y la consistencia del servicio.\n",
    "El servicio Zenda es un asistente conversacional impulsado por IA,\n",
    "orientado a personas que enfrentan desafíos personales, laborales o de desarrollo.\n",
    "Ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de\n",
    "adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas).\n",
    "Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes.\n",
    "\"\"\" # He añadido un poco más de texto, aunque sigue siendo corto para el caché efectivo.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(\"Intentando crear el recurso Context Cache en Vertex AI...\")\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    # Nota: cached_content_resource también tiene info como .size_bytes, .token_count, etc.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado.\")\n",
    "    print(\"- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\")\n",
    "    print(\"- El contenido es significativamente menor al tamaño mínimo para caching explícito (~4k tokens), aunque debería permitir crear el recurso igual.\")\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8104b223-7768-4832-a32c-bf48530c8663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI con el modelo 'gemini-2.5-pro-preview-05-06'...\n",
      "\n",
      "--- Error al crear el Context Cache ---\n",
      "Mensaje de error: 400 The cached content is of 157 tokens. The minimum token count to start caching is 1024.\n",
      "Posibles causas:\n",
      "- Permisos insuficientes para crear recursos de caching en tu proyecto.\n",
      "- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos).\n",
      "- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\n",
      "- El contenido es significativamente menor al tamaño mínimo para caching explícito (~4k tokens), aunque debería permitir crear el recurso igual.\n",
      "Si estás en Workbench, verifica que tienes permisos para Vertex AI.\n",
      "Intenta verificar la disponibilidad de modelos en la región 'us-central1' o prueba con otro nombre de modelo 2.5.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 2.5 que queremos usar para el caché\n",
    "# ¡Corregimos el nombre basándonos en tus fuentes y tu necesidad de usar 2.5!\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # Intentamos con este nombre de modelo 2.5\n",
    "\n",
    "\n",
    "# Contenido de ejemplo para cachear\n",
    "# Aquí podrías poner después una pauta, una definición, etc.\n",
    "# Recuerda que para que el caching explícito sea efectivo,\n",
    "# el contenido debe tener al menos ~4096 tokens.\n",
    "# Este es solo un ejemplo para probar la creación del recurso.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Las pautas generales de Zenda son protocolos de acompañamiento y reflexión.\n",
    "Están diseñadas para guiar la conversación del agente Zenda con el cliente.\n",
    "Incluyen acciones, formas de actuar (cómo) y objetivos (para qué).\n",
    "Son clave para asegurar la calidad y la consistencia del servicio.\n",
    "El servicio Zenda es un asistente conversacional impulsado por IA,\n",
    "orientado a personas que enfrentan desafíos personales, laborales o de desarrollo.\n",
    "Ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de\n",
    "adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas).\n",
    "Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes.\n",
    "\"\"\" # He añadido un poco más de texto, aunque sigue siendo corto para el caché efectivo.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(f\"Intentando crear el recurso Context Cache en Vertex AI con el modelo '{MODEL_NAME}'...\")\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    # Nota: cached_content_resource también tiene info como .size_bytes, .token_count, etc.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos).\")\n",
    "    print(\"- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\")\n",
    "    print(\"- El contenido es significativamente menor al tamaño mínimo para caching explícito (~4k tokens), aunque debería permitir crear el recurso igual.\")\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")\n",
    "    print(f\"Intenta verificar la disponibilidad de modelos en la región '{LOCATION}' o prueba con otro nombre de modelo 2.5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9120885b-695b-4924-81c1-5562ba8e17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI con el modelo 'gemini-2.5-pro-preview-05-06' y contenido más largo...\n",
      "\n",
      "--- Error al crear el Context Cache ---\n",
      "Mensaje de error: 400 The cached content is of 689 tokens. The minimum token count to start caching is 1024.\n",
      "Posibles causas:\n",
      "- Permisos insuficientes para crear recursos de caching en tu proyecto.\n",
      "- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos si el tamaño del contenido es correcto).\n",
      "- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\n",
      "- **El contenido ANTERIOR era muy corto, ¡este intento debería pasar ese error!**\n",
      "Si estás en Workbench, verifica que tienes permisos para Vertex AI.\n",
      "Intenta verificar la disponibilidad de modelos en la región 'us-central1' o prueba con otro nombre de modelo 2.5 si este falla por el modelo.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 2.5 que queremos usar para el caché\n",
    "# Usamos el nombre que pareció reconocido en el intento anterior\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "\n",
    "# Contenido para cachear - Fragmento más extenso del documento Zenda 7\n",
    "# Este texto está diseñado para superar los 1024 tokens mínimos requeridos.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Zenda es un servicio de asistencia conversacional impulsado por inteligencia artificial (IA), orientado a personas que enfrentan desafíos personales, laborales o de desarrollo. El servicio ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas) de acompañamiento, reflexión, observación emocional y evaluación del progreso. Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes. El sistema opera de manera autónoma, basado en un conjunto estructurado de pautas de intervención generales (con planes de incorporar pautas específicas post-MVP) y con mecanismos de calidad (Think Tool interno, QA post-sesión), seguridad y trazabilidad avanzados. Busca ofrecer una experiencia conversacional cálida, empática, útil y profesional.\n",
    "\n",
    "La arquitectura general (MVP) de Zenda se compone de un Backend Principal desplegado en Google Cloud Run, utilizando contenedores Docker y Python con el Google Agent Development Kit (ADK). La Base de Datos es Supabase (PostgreSQL), alojando toda la información persistente como clientes, entidades, pautas, bitácora, sesiones, temas y especialidades. Los Agentes de IA son orquestados por ADK, incluyendo el Agente DT (Director Técnico), el Agente Zenda (Conversacional Principal con Think Tool interno) y el Agente QA (para procesamiento post-sesión). La Interfaz de Usuario (MVP) es una aplicación Streamlit. Se integran Servicios de Voz con Gemini Speech-to-Text (STT) y Text-to-Speech (TTS). Para la gestión de contexto y estado, se utiliza ADK State para datos dinámicos de la sesión y Vertex AI Context Caching (planificado post-MVP según la sección 15, pero que estamos implementando) para datos más estáticos o grandes como Pautas, Especialidades y Resumen Histórico. El Estado de Sesión usa ADK State para datos específicos de la sesión como Entidades Activas, Criterios, Preferencias.\n",
    "\n",
    "La Gestión de Contexto, Estado y Memoria (MVP) es crucial. El ADK State (Session Scope) contiene información como Entidades Activas (personas, orgs, jargon, conceptos), Criterios Pautas (actualizados por DT), Tema Elegido, Especialidad Principal/Secundarias, Preferencias Usuario e información del turno actual. Se usa como memoria de trabajo rápida para la lógica de los agentes durante la sesión. El Vertex AI Context Caching está diseñado para contener Pautas Generales (completas), Definiciones de Especialidades y el Resumen Histórico Acumulativo. Su uso principal es proporcionar conocimiento de fondo y definiciones al LLM de Zenda sin sobrecargar el prompt directo, gestionado con TTLs apropiados. La Estrategia de Memoria Larga se basa en un resumen incremental/acumulativo generado post-sesión. Al finalizar una sesión, el proceso de QA invoca un LLM para leer la bitácora de la sesión y el resumen acumulativo previo, generando un nuevo resumen integrado que se guarda en un campo dedicado en la tabla `sesiones`. Al inicio de la siguiente sesión, el DT lee este resumen y lo carga en Context Caching (en el plan original, ahora adaptando al uso directo del recurso de caching que estamos creando).\n",
    "\"\"\" # Este texto combinado debería superar los 1024 tokens.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(f\"Intentando crear el recurso Context Cache en Vertex AI con el modelo '{MODEL_NAME}' y contenido más largo...\")\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    print(f\"Tamaño del contenido cacheado (tokens): {cached_content_resource.token_count}\")\n",
    "    print(f\"Tamaño del contenido cacheado (bytes): {cached_content_resource.size_bytes}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos si el tamaño del contenido es correcto).\")\n",
    "    print(\"- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\")\n",
    "    print(\"- **El contenido ANTERIOR era muy corto, ¡este intento debería pasar ese error!**\") # Aclaramos que el error anterior ya no debería ocurrir\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")\n",
    "    print(f\"Intenta verificar la disponibilidad de modelos en la región '{LOCATION}' o prueba con otro nombre de modelo 2.5 si este falla por el modelo.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0011bf-cb77-4748-a375-48b1824e9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI con el modelo 'gemini-2.5-pro-preview-05-06' y contenido más largo...\n",
      "\n",
      "--- ¡Recurso Context Cache creado exitosamente! ---\n",
      "Nombre del recurso: 800647874672066560\n",
      "Fecha de creación: 2025-05-16 19:56:57.351420+00:00\n",
      "Fecha de expiración: 2025-05-16 20:56:57.331324+00:00\n",
      "Modelo asociado: projects/zenda-adk/locations/us-central1/publishers/google/models/gemini-2.5-pro-preview-05-06\n",
      "\n",
      "--- Error al crear el Context Cache ---\n",
      "Mensaje de error: 'CachedContent' object has no attribute 'token_count'\n",
      "Posibles causas:\n",
      "- Permisos insuficientes para crear recursos de caching en tu proyecto.\n",
      "- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos si el tamaño del contenido es correcto).\n",
      "- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\n",
      "- **El contenido ANTERIOR era muy corto, ¡este intento debería pasar ese error!**\n",
      "Si estás en Workbench, verifica que tienes permisos para Vertex AI.\n",
      "Intenta verificar la disponibilidad de modelos en la región 'us-central1' o prueba con otro nombre de modelo 2.5 si este falla por el modelo.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 2.5 que queremos usar para el caché\n",
    "# Usamos el nombre que pareció reconocido en el intento anterior\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "\n",
    "# Contenido para cachear - Fragmento MUCHO más extenso del documento Zenda 7\n",
    "# Este texto está diseñado para superar HOLGADAMENTE los 1024 tokens mínimos requeridos.\n",
    "# Incluye partes de la introducción, arquitectura, gestión de contexto, manejo de pautas, temas, entidades y ajustes del MVP.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Zenda es un servicio de asistencia conversacional impulsado por inteligencia artificial (IA), orientado a personas que enfrentan desafíos personales, laborales o de desarrollo. El servicio ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas) de acompañamiento, reflexión, observación emocional y evaluación del progreso. Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes. El sistema opera de manera autónoma, basado en un conjunto estructurado de pautas de intervención generales (con planes de incorporar pautas específicas post-MVP) y con mecanismos de calidad (Think Tool interno, QA post-sesión), seguridad y trazabilidad avanzados. Busca ofrecer una experiencia conversacional cálida, empática, útil y profesional.\n",
    "\n",
    "La arquitectura general (MVP) de Zenda se compone de un Backend Principal desplegado en Google Cloud Run, utilizando contenedores Docker y Python con el Google Agent Development Kit (ADK). La Base de Datos es Supabase (PostgreSQL), alojando toda la información persistente como clientes, entidades, pautas, bitácora, sesiones, temas y especialidades. Los Agentes de IA son orquestados por ADK. Para el MVP, la secuencia principal involucra al Agente DT (Director Técnico - LlmAgent), el Agente Zenda (Agente Conversacional Principal - LlmAgent con Think Tool interno) y el Agente QA (Agente de Calidad - Lógica post-sesión, LlmAgent avanzado). La Interfaz de Usuario (MVP) es una aplicación en Streamlit para interacción vía chat de texto y voz. Se integran Servicios de Voz con Gemini Speech-to-Text (STT) y Text-to-Speech (TTS). Para la gestión de contexto y estado, se utiliza ADK State (Session Scope) para datos dinámicos y Vertex AI Context Caching para datos estables/grandes (Pautas, Especialidades, Resumen Histórico), aunque la implementación explícita de este último fue ajustada para el MVP según la Sección 15.\n",
    "\n",
    "La Gestión de Contexto, Estado y Memoria (MVP) es crucial para el desempeño de Zenda. El ADK State (Session Scope) almacena contenido como Entidades Activas (personas, orgs, jargon, conceptos), Criterios Pautas (actualizados por DT cada turno), Tema Elegido, Especialidad Principal/Secundarias, Preferencias Usuario (leídas de clientes) e Información del turno actual (emoción detectada, flags de riesgo básicos). Se usa como memoria de trabajo rápida para la lógica de los agentes (DT, Zenda) durante la sesión, actualizándose dinámicamente. El Vertex AI Context Caching está diseñado para contener Contenido como Pautas Generales (completas, desde Supabase), Definiciones de Especialidades (~35, desde Supabase) y el Resumen Histórico Acumulativo (última versión pre-calculada, desde Supabase sesiones.historical_summary). Su uso es para contexto más estático y/o voluminoso cargado al inicio de sesión, consumido principalmente por el LLM de Zenda para proporcionar conocimiento de fondo y definiciones sin sobrecargar el prompt directo, gestionado con TTLs apropiados. La Estrategia de Memoria Larga se adopta mediante el enfoque de resumen incremental/acumulativo generado post-sesión. Al final de la sesión N, el proceso de QA Post-Sesión invoca un LLM económico para leer la bitácora de la sesión N y el resumen acumulativo de N-1, generando un nuevo resumen acumulativo N que integra ambos. Este nuevo resumen se guarda en un campo dedicado (ej., historical_summary de tipo TEXT o JSONB) en la tabla `sesiones` asociada a la sesión N. Al inicio de la sesión N+1, DT (vía retrieve_summary_tool) lee este campo de la sesión N y lo carga en Context Caching (en el plan original antes de los ajustes del MVP).\n",
    "\n",
    "El Manejo de Pautas (MVP) es fundamental. El MVP operará con el conjunto existente de ~166 pautas generales transversales (archivo Pautas.csv). Estas pautas deben ser validadas por expertos antes de usarse en el MVP. Se implementará el Mecanismo de Adherencia Obligatoria: lógica explícita en Zenda para identificar la pauta general más relevante, extraer campos clave (Accion, Como, Para) y generar un borrador de respuesta intentando cumplir estrictamente con el Como para lograr el Para. También se preparará la declaración de la pauta usada para QA. Las Pautas Pendientes incluyen desarrollar e implementar nuevas pautas específicas para guiar la fase inicial de cada sesión (Acuerdo de Sesión). Las pautas generales validadas se almacenarán en la tabla `pautas` de Supabase.\n",
    "\n",
    "El Manejo de Temas y Especialidades (MVP) implica Definiciones: Tema (problema/objetivo del cliente) y Especialidad (enfoque profesional Zenda, ~35 tipos). El flujo de inicio de sesión implica que Cliente expone, Zenda/DT ayudan a elegir Tema, DT consulta tabla temas (vía Tool) para obtener Especialidad(es) recomendadas, y DT actualiza State. Zenda usa Especialidad(es) del State para adaptar su prompt/estilo/persona base y ayudar a contextualizar la selección de pautas generales.\n",
    "\n",
    "El Manejo de Entidades y Contexto Específico (MVP) usa la tabla `entidades`. Su propósito MVP es almacenar actores relacionales (personas, orgs) Y contexto específico del cliente (Jerga, Conceptos, Proyectos Mencionados, etc.), no para Progreso/Objetivos en MVP. La estructura final (MVP) se basa en la imagen image_d0421e.png. Se usa tipo_entidad Extendido añadiendo valores como \"Jargon\", \"Concepto\", \"ProyectoCliente\". La Coordinación entidades_tool <-> ADK State implica que DT carga entidades activas iniciales en State, Zenda lee primero del State, entidades_tool maneja lectura/escritura en Supabase, y tras una escritura exitosa, Zenda actualiza la entidad correspondiente en el State. La Búsqueda Contextual Interna LLM (Optimizada) implica que Zenda usa su conocimiento interno LLM \"on demand\" para definir/explicar términos/conceptos, guardando (vía tools) SOLO información muy específica del cliente o muy recurrente como entidad de tipo \"Jargon\" o \"Concepto\".\n",
    "\n",
    "Finalmente, la sección Ajustes al Alcance del MVP (Sección 15) detalla simplificaciones. En lugar de muchos agentes complejos, el MVP usa dos principales (DT y Zenda) con flujo secuencial fijo. Se eliminó el uso explícito del servicio Vertex AI Context Caching del alcance del MVP, basando el contexto de sesión en el ADK State y en incluir contexto relevante directamente en el prompt de ZendaAgentMVP (Implicit Caching). La persistencia de datos de sesión se simplificó a escrituras periódicas a una tabla intermedia y una escritura síncrona completa al final, difiriendo la pipeline asíncrona completa (Pub/Sub + Worker). Funcionalidades avanzadas como análisis emocional multimodal y detección de riesgos sofisticada se difieren, manteniendo solo interacción por voz STT/TTS y análisis emocional basado en texto. El Think Tool como callback con segundo LLM se difiere, basando la adherencia a pautas en el prompt de Zenda y la validación post-sesión del Agente QA, con DT alertando internamente. La implementación de la base de datos se enfocó solo en las tablas y campos indispensables para el MVP. El manejo de la tabla de entidades se simplificó a tipos básicos (Persona, Organizacion), difiriendo tipos extendidos como \"Jargon\" y relaciones detalladas. El mapeo Tema-Especialidad usa datos estáticos simples en lugar de consulta dinámica a tablas de Supabase. El front-end se enfoca en el núcleo conversacional. El registro de eventos en la bitácora se limita a eventos clave de alto nivel. El soporte multi-lenguaje es básico a nivel de infraestructura. El monitoreo se enfoca en logging estructurado de eventos clave. El proceso de despliegue es manual. El manejo de errores se enfoca en registro y robustez básica. Se mantiene el uso estratégico de Pydantic. El front-end en Streamlit es la prioridad 1 para el MVP.\n",
    "\"\"\" # Este texto combinado debería superar los 1024 tokens.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(f\"Intentando crear el recurso Context Cache en Vertex AI con el modelo '{MODEL_NAME}' y contenido más largo...\")\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    print(f\"Tamaño del contenido cacheado (tokens): {cached_content_resource.token_count}\")\n",
    "    print(f\"Tamaño del contenido cacheado (bytes): {cached_content_resource.size_bytes}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado (este es el problema más probable si no es permisos si el tamaño del contenido es correcto).\")\n",
    "    print(\"- Problemas de conexión o configuración de red (ej. VPC Service Controls si aplica).\")\n",
    "    print(\"- **El contenido ANTERIOR era muy corto, ¡este intento debería pasar ese error!**\")\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")\n",
    "    print(f\"Intenta verificar la disponibilidad de modelos en la región '{LOCATION}' o prueba con otro nombre de modelo 2.5 si este falla por el modelo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e92363f-2014-4d59-9653-ea93c5848f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asegurando inicialización de Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando usar el caché '2072914769404231680' en una llamada al modelo 'gemini-2.5-pro-preview-05-06'...\n",
      "\n",
      "--- Error al usar el Context Cache en la llamada al modelo ---\n",
      "Mensaje de error: _GenerativeModel.generate_content() got an unexpected keyword argument 'cached_content'\n",
      "Posibles causas:\n",
      "- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\n",
      "- Problemas de conexión o configuración. Asegúrate de que estás en 'us-central1'.\n",
      "- Error en el nombre del recurso de caché o del modelo.\n",
      "- El prompt o la combinación con el caché causaron un error interno del modelo.\n",
      "Nombre del recurso de caché usado: 2072914769404231680\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# Asegúrate de que estas variables estén definidas,\n",
    "# por ejemplo, si corriste la celda anterior con éxito, ya lo están.\n",
    "# Si no, descomenta y establece sus valores correctos:\n",
    "# PROJECT_ID = 'zenda-adk'\n",
    "# LOCATION = \"us-central1\"\n",
    "# MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "# ¡Importante! Necesitamos el objeto 'cached_content_resource'\n",
    "# que fue creado en la celda anterior y contiene el nombre del recurso de caché.\n",
    "# Si corriste la celda anterior exitosamente, la variable `cached_content_resource` existe.\n",
    "# Su nombre único que necesitamos para referenciarlo es: cached_content_resource.name\n",
    "# Si corrieras esta celda *sin* haber corrido la anterior,\n",
    "# necesitarías obtener el recurso de caché de otra manera, por ejemplo:\n",
    "# from vertexai.preview import caching\n",
    "# resource_name = \"projects/zenda-adk/locations/us-central1/cachedContents/EL_ID_QUE_TE_DIO_ANTES\" # Reemplaza con el ID real si es necesario\n",
    "# cached_content_resource = caching.CachedContent(resource_name)\n",
    "\n",
    "\n",
    "# --- Inicializar Vertex AI (Aunque ya inicializado en la celda anterior,\n",
    "#     es buena práctica incluir la inicialización si corres esta celda por separado) ---\n",
    "try:\n",
    "    # Usamos os.environ.get por si ya están definidas como variables de entorno\n",
    "    current_project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', PROJECT_ID)\n",
    "    current_location = os.environ.get('GOOGLE_CLOUD_LOCATION', LOCATION)\n",
    "    print(f\"Asegurando inicialización de Vertex AI para el proyecto '{current_project_id}' en la región '{current_location}'...\")\n",
    "    vertexai.init(project=current_project_id, location=current_location)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    raise # Relanzamos la excepción\n",
    "\n",
    "\n",
    "# --- Usar el Context Cache en una llamada al modelo ---\n",
    "try:\n",
    "    print(f\"Intentando usar el caché '{cached_content_resource.name}' en una llamada al modelo '{MODEL_NAME}'...\")\n",
    "\n",
    "    # Inicializa el modelo Gemini 2.5\n",
    "    # Usamos el mismo nombre de modelo que se usó para crear el caché\n",
    "    model = GenerativeModel(model_name=MODEL_NAME)\n",
    "\n",
    "    # Define tu prompt. Este prompt es la \"pregunta\" o instrucción que le das al modelo,\n",
    "    # y el modelo usará el contenido cacheado como contexto de fondo.\n",
    "    prompt_usuario = \"\"\"\n",
    "    Basándote EXCLUSIVAMENTE en la información que te fue proporcionada como contexto (el contenido cacheado),\n",
    "    describe brevemente:\n",
    "    1. ¿Qué es Zenda?\n",
    "    2. ¿Cuáles son los 3 agentes principales que lo componen?\n",
    "    3. ¿Qué mecanismos de gestión de contexto y memoria se mencionan?\n",
    "    Responde de forma concisa.\n",
    "    \"\"\"\n",
    "\n",
    "    # Llama al modelo, referenciando el recurso de caché por su nombre.\n",
    "    # Aquí es donde le decimos al modelo \"usa este caché como parte del contexto\".\n",
    "    response = model.generate_content(\n",
    "        contents=[Part.from_text(prompt_usuario)], # Tu prompt aquí como una 'parte' del contenido\n",
    "        cached_content=cached_content_resource.name # Referencia al recurso de caché por su nombre\n",
    "        # Importante: Cuando usas cached_content, no puedes usar los parámetros\n",
    "        # system_instructions, tool_config o tools directamente en esta llamada.\n",
    "        # Si necesitas instrucciones del sistema, herramientas o configuración de herramientas,\n",
    "        # deben estar definidas DENTRO del contenido que pusiste en el Context Cache.\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Respuesta del modelo usando el Context Cache ---\")\n",
    "    print(response.text)\n",
    "\n",
    "    # Intenta ver si hay info de uso de caché en la respuesta.\n",
    "    # Esto nos puede confirmar cuántos tokens del caché se usaron.\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "         print(\"\\n--- Metadatos de uso (puede mostrar el uso del caché) ---\")\n",
    "         print(f\"Total tokens de entrada (prompt + caché): {response.usage_metadata.total_token_count}\")\n",
    "         # Estos atributos pueden variar o no estar presentes según la versión específica del SDK y modelo\n",
    "         try:\n",
    "             print(f\"Tokens cache usados: {response.usage_metadata.cached_content_token_count}\")\n",
    "         except AttributeError:\n",
    "             print(\"El atributo 'cached_content_token_count' no está disponible en los metadatos.\")\n",
    "         try:\n",
    "             print(f\"Tokens solo del prompt (no cache): {response.usage_metadata.prompt_token_count}\")\n",
    "         except AttributeError:\n",
    "             print(\"El atributo 'prompt_token_count' no está disponible en los metadatos.\")\n",
    "         try:\n",
    "             print(f\"Tokens de salida generados: {response.usage_metadata.candidates_token_count}\")\n",
    "         except AttributeError:\n",
    "             print(\"El atributo 'candidates_token_count' no está disponible en los metadatos.\")\n",
    "    else:\n",
    "        print(\"\\n--- No se encontraron metadatos de uso detallados en la respuesta ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al usar el Context Cache en la llamada al modelo ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\")\n",
    "    print(f\"- Problemas de conexión o configuración. Asegúrate de que estás en '{LOCATION}'.\")\n",
    "    print(\"- Error en el nombre del recurso de caché o del modelo.\")\n",
    "    print(\"- El prompt o la combinación con el caché causaron un error interno del modelo.\")\n",
    "    print(f\"Nombre del recurso de caché usado: {cached_content_resource.name if 'cached_content_resource' in locals() else 'variable cached_content_resource no definida'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a02fc9-fcd1-47ff-a2d0-d31d1eca0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asegurando inicialización de Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando usar el caché '2072914769404231680' en una llamada al modelo 'gemini-2.5-pro-preview-05-06'...\n",
      "\n",
      "--- Error al usar el Context Cache en la llamada al modelo ---\n",
      "Mensaje de error: GenerationConfig.__init__() got an unexpected keyword argument 'cached_content'\n",
      "Posibles causas:\n",
      "- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\n",
      "- Problemas de conexión o configuración. Asegúrate de que estás en 'us-central1'.\n",
      "- Error en el nombre del recurso de caché o del modelo.\n",
      "- El prompt o la combinación con el caché causaron un error interno del modelo.\n",
      "Nombre del recurso de caché intentado usar: 2072914769404231680\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "# Importamos GenerationConfig además de los otros\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# Estas variables deberían estar definidas si corriste la celda anterior.\n",
    "# Si por alguna razón no lo están, descomenta y ajusta los valores.\n",
    "# PROJECT_ID = 'zenda-adk'\n",
    "# LOCATION = \"us-central1\"\n",
    "# MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "# ¡Importante! Necesitamos el objeto 'cached_content_resource'\n",
    "# que fue creado en la celda anterior. Contiene el nombre del recurso de caché\n",
    "# (por ejemplo, 'projects/zenda-adk/locations/us-central1/cachedContents/2072914769404231680').\n",
    "# Si corriste la celda anterior con éxito, esta variable existe y contiene el recurso.\n",
    "\n",
    "\n",
    "# --- Inicializar Vertex AI (Precaución si corres esta celda sola) ---\n",
    "try:\n",
    "    # Intentamos leer de variables de entorno por si acaso\n",
    "    current_project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', PROJECT_ID)\n",
    "    current_location = os.environ.get('GOOGLE_CLOUD_LOCATION', LOCATION)\n",
    "    print(f\"Asegurando inicialización de Vertex AI para el proyecto '{current_project_id}' en la región '{current_location}'...\")\n",
    "    vertexai.init(project=current_project_id, location=current_location)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    raise # Relanzamos la excepción\n",
    "\n",
    "\n",
    "# --- Usar el Context Cache en una llamada al modelo ---\n",
    "try:\n",
    "    # Si la variable cached_content_resource no existe por alguna razón (ej. reiniciaste el kernel),\n",
    "    # tendrías que recuperarla o crearla de nuevo. Asumimos que sí existe.\n",
    "    if 'cached_content_resource' not in locals() or cached_content_resource is None:\n",
    "         print(\"Error: La variable 'cached_content_resource' no fue encontrada. Asegúrate de haber ejecutado la celda anterior con éxito.\")\n",
    "         # Podrías intentar recuperarla por nombre si sabes el ID:\n",
    "         # from vertexai.preview import caching\n",
    "         # resource_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents/EL_ID_QUE_TE_DIO_ANTES\"\n",
    "         # cached_content_resource = caching.CachedContent(resource_name=resource_name)\n",
    "         exit() # Detenemos la ejecución si no tenemos el recurso de caché\n",
    "\n",
    "    print(f\"Intentando usar el caché '{cached_content_resource.name}' en una llamada al modelo '{MODEL_NAME}'...\")\n",
    "\n",
    "    # Inicializa el modelo Gemini 2.5\n",
    "    # Usamos el mismo nombre de modelo que se usó para crear el caché\n",
    "    model = GenerativeModel(model_name=MODEL_NAME)\n",
    "\n",
    "    # Define tu prompt. Este prompt es la \"pregunta\" o instrucción que le das al modelo,\n",
    "    # y el modelo usará el contenido cacheado como contexto de fondo.\n",
    "    # El contenido cacheado actúa como si fuera una parte MUY GRANDE del prompt de sistema o contexto inicial.\n",
    "    prompt_usuario = \"\"\"\n",
    "    Basándote EXCLUSIVAMENTE en la información que te fue proporcionada como contexto (el contenido cacheado),\n",
    "    describe brevemente:\n",
    "    1. ¿Qué es Zenda?\n",
    "    2. ¿Cuáles son los 3 agentes principales que lo componen?\n",
    "    3. ¿Qué mecanismos de gestión de contexto y memoria se mencionan?\n",
    "    Responde de forma concisa.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ¡Aquí está la corrección! ---\n",
    "    # Creamos un objeto GenerationConfig\n",
    "    generation_config = GenerationConfig(\n",
    "        cached_content=cached_content_resource.name # Pasamos el nombre del recurso de caché aquí dentro\n",
    "        # Puedes añadir otros parámetros de generación si quieres, como temperature, max_output_tokens, etc.\n",
    "        # Por ejemplo: temperature=0.2, max_output_tokens=100\n",
    "    )\n",
    "\n",
    "    # Llama al modelo, pasando el prompt y el objeto de configuración\n",
    "    response = model.generate_content(\n",
    "        contents=[Part.from_text(prompt_usuario)], # Tu prompt aquí como una lista de 'partes'\n",
    "        generation_config=generation_config # Pasamos el objeto de configuración completa\n",
    "        # NOTA: No puedes usar system_instructions, tools, o tool_config DIRECTAMENTE\n",
    "        # en esta llamada si estás usando `cached_content`. Esos deben estar\n",
    "        # definidos *dentro* del contenido que pusiste en el Context Cache si los necesitas aplicar al caché.\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Respuesta del modelo usando el Context Cache ---\")\n",
    "    # El texto de la respuesta está en response.text\n",
    "    print(response.text)\n",
    "\n",
    "    # Intentamos ver si hay info de uso de caché en la respuesta.\n",
    "    # Esto puede confirmar cuántos tokens del caché se usaron (si el SDK lo expone).\n",
    "    print(\"\\n--- Intentando mostrar metadatos de uso ---\")\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "         print(\"Metadatos de uso encontrados:\")\n",
    "         # Imprimimos todos los atributos disponibles en usage_metadata para investigar\n",
    "         for attr in dir(response.usage_metadata):\n",
    "             if not attr.startswith('_'): # Ignorar atributos internos de Python\n",
    "                 try:\n",
    "                     value = getattr(response.usage_metadata, attr)\n",
    "                     print(f\"- {attr}: {value}\")\n",
    "                 except Exception:\n",
    "                     print(f\"- {attr}: [No se pudo obtener el valor]\")\n",
    "\n",
    "    else:\n",
    "        print(\"No se encontraron metadatos de uso detallados en la respuesta.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al usar el Context Cache en la llamada al modelo ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\")\n",
    "    print(f\"- Problemas de conexión o configuración. Asegúrate de que estás en '{LOCATION}'.\")\n",
    "    print(\"- Error en el nombre del recurso de caché o del modelo.\")\n",
    "    print(\"- El prompt o la combinación con el caché causaron un error interno del modelo.\")\n",
    "    # Intentamos imprimir el nombre del caché usado si la variable existe\n",
    "    cache_name_to_print = \"variable cached_content_resource no definida\"\n",
    "    if 'cached_content_resource' in locals() and cached_content_resource:\n",
    "         try:\n",
    "             cache_name_to_print = cached_content_resource.name\n",
    "         except Exception:\n",
    "              cache_name_to_print = \"objeto cached_content_resource existe pero sin .name\"\n",
    "\n",
    "    print(f\"Nombre del recurso de caché intentado usar: {cache_name_to_print}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394103a3-404c-4202-9123-903f72844618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicializando cliente google.genai...\n",
      "Error al inicializar cliente google.genai: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.\n",
      "Asegúrate de que tu entorno esté autenticado con GCP y que tengas permisos de Vertex AI.\n",
      "Si estás fuera de GCP (ej. en tu máquina local), quizás necesites configurar GOOGLE_APPLICATION_CREDENTIALS.\n",
      "Intentando usar el caché '2072914769404231680' en una llamada al modelo 'gemini-2.5-pro-preview-05-06' usando cliente genai...\n",
      "Llamando al modelo generate_content con caché...\n",
      "\n",
      "--- Error al usar el Context Cache en la llamada al modelo (cliente google.genai) ---\n",
      "Mensaje de error: name 'genai_client' is not defined\n",
      "Posibles causas:\n",
      "- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\n",
      "- Problemas de conexión o configuración. Asegúrate de que estás en 'us-central1'.\n",
      "- Error en el nombre del recurso de caché o del modelo.\n",
      "- El prompt, el caché o la combinación causaron un error interno del modelo.\n",
      "Asegúrate de que el cliente genai está inicializado y autenticado correctamente.\n",
      "Nombre del recurso de caché intentado usar: 2072914769404231680\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Importamos desde google.genai ahora los módulos necesarios\n",
    "import google.genai\n",
    "from google.genai import types # Necesitamos types para GenerateContentConfig\n",
    "\n",
    "# --- Configuración ---\n",
    "# Estas variables deberían estar definidas si corriste la celda anterior.\n",
    "# Si por alguna razón no lo están (ej. corres esta celda sola y reiniciaste el kernel),\n",
    "# descomenta y establece sus valores correctos.\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # El modelo usado para crear el caché\n",
    "\n",
    "# --- Manejo del recurso de caché creado previamente ---\n",
    "# ¡Importante! Necesitamos el objeto 'cached_content_resource'\n",
    "# que fue creado en la celda anterior. Este objeto contiene el nombre del recurso de caché.\n",
    "\n",
    "# --- Instrucción detallada: Cómo asegurar que 'cached_content_resource' esté disponible ---\n",
    "# OPCIÓN 1 (Recomendada - Si no reiniciaste el kernel):\n",
    "# Si acabas de ejecutar la celda del \"Paso 2\" con éxito en esta misma sesión de Jupyter\n",
    "# y no reiniciaste el kernel, la variable `cached_content_resource` ya debería existir\n",
    "# y estar lista para usar. No necesitas hacer nada en este bloque.\n",
    "\n",
    "# OPCIÓN 2 (Si reiniciaste el kernel o corres esta celda sola):\n",
    "# Si reiniciaste el kernel de Jupyter o estás ejecutando esta celda sin haber ejecutado la anterior\n",
    "# en esta sesión, la variable `cached_content_resource` NO existirá.\n",
    "# Para obtener el recurso de caché, podemos recuperarlo por su nombre COMPLETO\n",
    "# (que obtuviste en la salida exitosa del Paso 2, por ejemplo, 'projects/zenda-adk/locations/us-central1/cachedContents/EL_ID_NUMERICO').\n",
    "# Descomenta el siguiente bloque de código y reemplaza 'EL_ID_NUMERICO_DE_TU_CACHE' con el número de ID real de tu caché.\n",
    "# Esto recuperará el objeto del recurso de caché desde Vertex AI.\n",
    "\n",
    "# try:\n",
    "#     print(\"OPCIÓN 2: Intentando recuperar el recurso de caché por nombre...\")\n",
    "#     # Reemplaza 'EL_ID_NUMERICO_DE_TU_CACHE' con el número ID que obtuviste (ej. 2072914769404231680)\n",
    "#     cache_resource_id = '2072914769404231680' # <<< ¡REEMPLAZA SI TU ID ES DIFERENTE!\n",
    "#     cache_resource_name_str = f\"projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents/{cache_resource_id}\"\n",
    "#\n",
    "#     # Para recuperar el recurso con google.genai, necesitamos un cliente genai.\n",
    "#     # Este cliente intentará usar las credenciales por defecto de tu entorno (como en Workbench).\n",
    "#     genai_client_for_retrieval = google.genai.Client()\n",
    "#\n",
    "#     cached_content_resource = genai_client_for_retrieval.get_cached_content(name=cache_resource_name_str)\n",
    "#     print(f\"Recurso de caché '{cache_resource_name_str}' recuperado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error al recuperar el recurso de caché por nombre: {e}\")\n",
    "#     print(\"Asegúrate de que el nombre del recurso sea correcto, que el caché no haya expirado y que tengas permisos para acceder.\")\n",
    "#     # Si falla la recuperación, detenemos la ejecución\n",
    "#     exit() # Detiene la ejecución de la celda\n",
    "\n",
    "\n",
    "# Verificamos si la variable existe después de intentar la OPCIÓN 1 o la OPCIÓN 2\n",
    "if 'cached_content_resource' not in locals() or cached_content_resource is None:\n",
    "     print(\"\\n--- ERROR FATAL: La variable 'cached_content_resource' NO está disponible. ---\")\n",
    "     print(\"Necesitas ejecutar primero la celda del 'Paso 2' (Intento con Contenido Largo) CON ÉXITO\")\n",
    "     print(\"o usar la OPCIÓN 2 de recuperación de caché dentro de esta celda si reiniciaste el kernel.\")\n",
    "     exit() # Detiene la ejecución si el recurso no está listo\n",
    "\n",
    "\n",
    "# --- Inicializar Cliente google.genai ---\n",
    "# Usaremos google.genai.Client ahora. Este cliente intentará usar las credenciales\n",
    "# de aplicación por defecto de tu entorno (que deberían funcionar en Vertex AI Workbench).\n",
    "try:\n",
    "    print(f\"\\nInicializando cliente google.genai...\")\n",
    "    genai_client = google.genai.Client()\n",
    "    # El cliente genai intentará inferir el proyecto y ubicación de las credenciales o variables de entorno.\n",
    "    # No siempre es necesario configurar explícitamente el endpoint si las credenciales de GCP están bien puestas.\n",
    "    print(\"Cliente google.genai inicializado.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar cliente google.genai: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que tengas permisos de Vertex AI.\")\n",
    "    print(\"Si estás fuera de GCP (ej. en tu máquina local), quizás necesites configurar GOOGLE_APPLICATION_CREDENTIALS.\")\n",
    "    exit() # Detenemos la ejecución si el cliente falla\n",
    "\n",
    "\n",
    "# --- Usar el Context Cache en una llamada al modelo usando el cliente genai ---\n",
    "try:\n",
    "    print(f\"Intentando usar el caché '{cached_content_resource.name}' en una llamada al modelo '{MODEL_NAME}' usando cliente genai...\")\n",
    "\n",
    "    # Define tu prompt.\n",
    "    prompt_usuario = \"\"\"\n",
    "    Basándote EXCLUSIVAMENTE en la información que te fue proporcionada como contexto (el contenido cacheado),\n",
    "    describe brevemente:\n",
    "    1. ¿Qué es Zenda?\n",
    "    2. ¿Cuáles son los 3 agentes principales que lo componen?\n",
    "    3. ¿Qué mecanismos de gestión de contexto y memoria se mencionan?\n",
    "    Responde de forma concisa.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ¡Aquí usamos la configuración con el cliente genai! ---\n",
    "    # Creamos un objeto GenerateContentConfig (usando types de google.genai)\n",
    "    # Y le pasamos el nombre completo del recurso de caché.\n",
    "    generation_config = types.GenerateContentConfig(\n",
    "        cached_content=cached_content_resource.name # Pasamos el nombre completo del recurso de caché aquí dentro\n",
    "        # Puedes añadir otros parámetros de generación si quieres\n",
    "        # temperature=0.2, max_output_tokens=200 # Ejemplo de otros parámetros\n",
    "    )\n",
    "\n",
    "    # Llama al modelo usando el cliente genai.models.generate_content.\n",
    "    # Para este cliente, el nombre del modelo se pasa con el path completo del recurso en Vertex AI.\n",
    "    full_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_NAME}\"\n",
    "\n",
    "    print(\"Llamando al modelo generate_content con caché...\")\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=full_model_name, # Nombre completo del modelo como recurso de Vertex AI\n",
    "        contents=[{\"text\": prompt_usuario}], # El prompt como una lista de dicts con clave 'text'\n",
    "        generation_config=generation_config # Pasamos el objeto de configuración\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Respuesta del modelo usando el Context Cache (cliente google.genai) ---\")\n",
    "    # La respuesta del texto suele estar en response.candidates[0].content.parts[0].text\n",
    "    try:\n",
    "        response_text = response.candidates[0].content.parts[0].text\n",
    "        print(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo extraer el texto de la respuesta fácilmente: {e}\")\n",
    "        print(\"Estructura completa de la respuesta para depurar:\")\n",
    "        print(response)\n",
    "\n",
    "\n",
    "    # Intenta ver si hay info de uso de caché en la respuesta.\n",
    "    # La metadata de uso suele estar en response.usage_metadata\n",
    "    print(\"\\n--- Intentando mostrar metadatos de uso (cliente google.genai) ---\")\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "         print(\"Metadatos de uso encontrados:\")\n",
    "         # Imprimimos todos los atributos disponibles en usage_metadata\n",
    "         # El conteo de tokens del caché usado debería aparecer aquí si la llamada lo reporta\n",
    "         for attr in dir(response.usage_metadata):\n",
    "             if not attr.startswith('_'): # Ignorar atributos internos de Python\n",
    "                 try:\n",
    "                     value = getattr(response.usage_metadata, attr)\n",
    "                     print(f\"- {attr}: {value}\")\n",
    "                 except Exception:\n",
    "                     print(f\"- {attr}: [No se pudo obtener el valor]\")\n",
    "\n",
    "    else:\n",
    "        print(\"No se encontraron metadatos de uso detallados en la respuesta.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al usar el Context Cache en la llamada al modelo (cliente google.genai) ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\")\n",
    "    print(f\"- Problemas de conexión o configuración. Asegúrate de que estás en '{LOCATION}'.\")\n",
    "    print(\"- Error en el nombre del recurso de caché o del modelo.\")\n",
    "    print(\"- El prompt, el caché o la combinación causaron un error interno del modelo.\")\n",
    "    print(\"Asegúrate de que el cliente genai está inicializado y autenticado correctamente.\")\n",
    "    # Intentamos imprimir el nombre del caché intentado usar\n",
    "    cache_name_to_print = \"variable cached_content_resource no definida\"\n",
    "    if 'cached_content_resource' in locals() and cached_content_resource:\n",
    "         try:\n",
    "             cache_name_to_print = cached_content_resource.name\n",
    "         except Exception:\n",
    "              cache_name_to_print = \"objeto cached_content_resource existe pero sin .name\"\n",
    "    print(f\"Nombre del recurso de caché intentado usar: {cache_name_to_print}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82ac6fc-58b0-4307-92b3-1d3d03721c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ERROR FATAL: La variable 'cached_content_resource' NO está disponible. ---\n",
      "Necesitas ejecutar primero la celda del 'Paso 2' (Intento con Contenido Largo) CON ÉXITO\n",
      "o usar la OPCIÓN 2 de recuperación de caché dentro de esta celda si reiniciaste el kernel.\n",
      "\n",
      "Inicializando cliente google.genai para proyecto 'zenda-adk' en región 'us-central1'...\n",
      "Error al inicializar cliente google.genai: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.\n",
      "Asegúrate de que tu entorno esté autenticado con GCP y que tengas permisos de Vertex AI.\n",
      "Si estás fuera de GCP, verifica GOOGLE_APPLICATION_CREDENTIALS o GOOGLE_API_KEY.\n",
      "\n",
      "--- Error al usar el Context Cache en la llamada al modelo (cliente google.genai) ---\n",
      "Mensaje de error: name 'cached_content_resource' is not defined\n",
      "Posibles causas:\n",
      "- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\n",
      "- Problemas de conexión o configuración. Asegúrate de que estás en 'us-central1'.\n",
      "- Error en el nombre del recurso de caché o del modelo.\n",
      "- El prompt, el caché o la combinación causaron un error interno del modelo.\n",
      "Asegúrate de que el cliente genai está inicializado y autenticado correctamente.\n",
      "Nombre del recurso de caché intentado usar: variable cached_content_resource no definida\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Importamos desde google.genai ahora los módulos necesarios\n",
    "import google.genai\n",
    "from google.genai import types # Necesitamos types para GenerateContentConfig\n",
    "import vertexai # Mantenemos vertexai importado por si acaso, aunque el cliente principal ahora sea genai\n",
    "\n",
    "# --- Configuración ---\n",
    "# Estas variables deberían estar definidas si corriste la celda anterior.\n",
    "# Si por alguna razón no lo están (ej. corres esta celda sola y reiniciaste el kernel),\n",
    "# descomenta y establece sus valores correctos.\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # El modelo usado para crear el caché\n",
    "\n",
    "# --- Manejo del recurso de caché creado previamente ---\n",
    "# ¡Importante! Necesitamos el objeto 'cached_content_resource'\n",
    "# que fue creado en la celda anterior. Este objeto contiene el nombre del recurso de caché.\n",
    "\n",
    "# --- Instrucción detallada: Cómo asegurar que 'cached_content_resource' esté disponible ---\n",
    "# OPCIÓN 1 (Recomendada - Si no reiniciaste el kernel):\n",
    "# Si acabas de ejecutar la celda del \"Paso 2\" con éxito en esta misma sesión de Jupyter\n",
    "# y no reiniciaste el kernel, la variable `cached_content_resource` ya debería existir\n",
    "# y estar lista para usar. No necesitas hacer nada en este bloque.\n",
    "\n",
    "# OPCIÓN 2 (Si reiniciaste el kernel o corres esta celda sola):\n",
    "# Si reiniciaste el kernel de Jupyter o estás ejecutando esta celda sin haber ejecutado la anterior\n",
    "# en esta sesión, la variable `cached_content_resource` NO existirá.\n",
    "# Para obtener el recurso de caché, podemos recuperarlo por su nombre COMPLETO\n",
    "# (que obtuviste en la salida exitosa del Paso 2, por ejemplo, 'projects/zenda-adk/locations/us-central1/cachedContents/EL_ID_NUMERICO').\n",
    "# Descomenta el siguiente bloque de código y reemplaza 'EL_ID_NUMERICO_DE_TU_CACHE' con el número de ID real de tu caché.\n",
    "# Esto recuperará el objeto del recurso de caché desde Vertex AI.\n",
    "\n",
    "# try:\n",
    "#     print(\"OPCIÓN 2: Intentando recuperar el recurso de caché por nombre...\")\n",
    "#     # Reemplaza 'EL_ID_NUMERICO_DE_TU_CACHE' con el número ID que obtuviste (ej. 2072914769404231680)\n",
    "#     cache_resource_id = '2072914769404231680' # <<< ¡REEMPLAZA SI TU ID ES DIFERENTE!\n",
    "#     cache_resource_name_str = f\"projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents/{cache_resource_id}\"\n",
    "#\n",
    "#     # Para recuperar el recurso con google.genai, necesitamos un cliente genai.\n",
    "#     # Este cliente intentará usar las credenciales por defecto de tu entorno (como en Workbench).\n",
    "#     genai_client_for_retrieval = google.genai.Client() # El cliente no necesita project/location para get_cached_content si las credenciales están bien\n",
    "#\n",
    "#     cached_content_resource = genai_client_for_retrieval.get_cached_content(name=cache_resource_name_str)\n",
    "#     print(f\"Recurso de caché '{cache_resource_name_str}' recuperado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error al recuperar el recurso de caché por nombre: {e}\")\n",
    "#     print(\"Asegúrate de que el nombre del recurso sea correcto, que el caché no haya expirado y que tengas permisos para acceder.\")\n",
    "#     # Si falla la recuperación, detenemos la ejecución\n",
    "#     exit() # Detiene la ejecución de la celda\n",
    "\n",
    "\n",
    "# Verificamos si la variable existe después de intentar la OPCIÓN 1 o la OPCIÓN 2\n",
    "if 'cached_content_resource' not in locals() or cached_content_resource is None:\n",
    "     print(\"\\n--- ERROR FATAL: La variable 'cached_content_resource' NO está disponible. ---\")\n",
    "     print(\"Necesitas ejecutar primero la celda del 'Paso 2' (Intento con Contenido Largo) CON ÉXITO\")\n",
    "     print(\"o usar la OPCIÓN 2 de recuperación de caché dentro de esta celda si reiniciaste el kernel.\")\n",
    "     exit() # Detiene la ejecución si el recurso no está listo\n",
    "\n",
    "\n",
    "# --- Inicializar Cliente google.genai (¡CORREGIDO!) ---\n",
    "# Usaremos google.genai.Client ahora y le pasamos el proyecto y la ubicación\n",
    "try:\n",
    "    print(f\"\\nInicializando cliente google.genai para proyecto '{PROJECT_ID}' en región '{LOCATION}'...\")\n",
    "    # ¡CORRECCIÓN CLAVE! Pasamos project y location para que sepa conectarse a Vertex AI en tu GCP.\n",
    "    genai_client = google.genai.Client(\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION\n",
    "        # También puedes intentar con: client_options={'api_endpoint': f\"{LOCATION}-aiplatform.googleapis.com\"}\n",
    "        # pero pasar project/location suele ser más directo según el error.\n",
    "    )\n",
    "    print(\"Cliente google.genai inicializado correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar cliente google.genai: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que tengas permisos de Vertex AI.\")\n",
    "    print(\"Si estás fuera de GCP, verifica GOOGLE_APPLICATION_CREDENTIALS o GOOGLE_API_KEY.\")\n",
    "    exit() # Detenemos la ejecución si el cliente falla\n",
    "\n",
    "\n",
    "# --- Usar el Context Cache en una llamada al modelo usando el cliente genai ---\n",
    "try:\n",
    "    print(f\"Intentando usar el caché '{cached_content_resource.name}' en una llamada al modelo '{MODEL_NAME}' usando cliente genai...\")\n",
    "\n",
    "    # Define tu prompt.\n",
    "    prompt_usuario = \"\"\"\n",
    "    Basándote EXCLUSIVAMENTE en la información que te fue proporcionada como contexto (el contenido cacheado),\n",
    "    describe brevemente:\n",
    "    1. ¿Qué es Zenda?\n",
    "    2. ¿Cuáles son los 3 agentes principales que lo componen?\n",
    "    3. ¿Qué mecanismos de gestión de contexto y memoria se mencionan?\n",
    "    Responde de forma concisa.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos un objeto GenerateContentConfig (usando types de google.genai)\n",
    "    # Y le pasamos el nombre completo del recurso de caché.\n",
    "    # Este objeto se pasará al parámetro 'config' de generate_content.\n",
    "    generation_config = types.GenerateContentConfig(\n",
    "        cached_content=cached_content_resource.name # Pasamos el nombre completo del recurso de caché aquí\n",
    "        # Puedes añadir otros parámetros de generación si quieres\n",
    "        # temperature=0.2, max_output_tokens=200 # Ejemplo de otros parámetros\n",
    "    )\n",
    "\n",
    "    # Llama al modelo usando el cliente genai.models.generate_content.\n",
    "    # Para este cliente, el nombre del modelo se pasa con el path completo del recurso en Vertex AI.\n",
    "    full_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_NAME}\"\n",
    "\n",
    "    print(\"Llamando al modelo generate_content con caché...\")\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=full_model_name, # Nombre completo del modelo como recurso de Vertex AI\n",
    "        contents=[{\"text\": prompt_usuario}], # El prompt como una lista de dicts con clave 'text'\n",
    "        config=generation_config # Pasamos el objeto de configuración AHORA sí al parámetro 'config'\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Respuesta del modelo usando el Context Cache (cliente google.genai) ---\")\n",
    "    # La respuesta del texto suele estar en response.candidates[0].content.parts[0].text\n",
    "    try:\n",
    "        response_text = response.candidates[0].content.parts[0].text\n",
    "        print(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo extraer el texto de la respuesta fácilmente: {e}\")\n",
    "        print(\"Estructura completa de la respuesta para depurar:\")\n",
    "        print(response)\n",
    "\n",
    "\n",
    "    # Intenta ver si hay info de uso de caché en la respuesta.\n",
    "    # La metadata de uso suele estar en response.usage_metadata\n",
    "    print(\"\\n--- Intentando mostrar metadatos de uso (cliente google.genai) ---\")\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "         print(\"Metadatos de uso encontrados:\")\n",
    "         # Imprimimos todos los atributos disponibles en usage_metadata\n",
    "         # El conteo de tokens del caché usado debería aparecer aquí si la llamada lo reporta\n",
    "         for attr in dir(response.usage_metadata):\n",
    "             if not attr.startswith('_'): # Ignorar atributos internos de Python\n",
    "                 try:\n",
    "                     value = getattr(response.usage_metadata, attr)\n",
    "                     print(f\"- {attr}: {value}\")\n",
    "                 except Exception:\n",
    "                     print(f\"- {attr}: [No se pudo obtener el valor]\")\n",
    "\n",
    "    else:\n",
    "        print(\"No se encontraron metadatos de uso detallados en la respuesta.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al usar el Context Cache en la llamada al modelo (cliente google.genai) ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\")\n",
    "    print(f\"- Problemas de conexión o configuración. Asegúrate de que estás en '{LOCATION}'.\")\n",
    "    print(\"- Error en el nombre del recurso de caché o del modelo.\")\n",
    "    print(\"- El prompt, el caché o la combinación causaron un error interno del modelo.\")\n",
    "    print(\"Asegúrate de que el cliente genai está inicializado y autenticado correctamente.\")\n",
    "    # Intentamos imprimir el nombre del caché intentado usar\n",
    "    cache_name_to_print = \"variable cached_content_resource no definida\"\n",
    "    if 'cached_content_resource' in locals() and cached_content_resource:\n",
    "         try:\n",
    "             cache_name_to_print = cached_content_resource.name\n",
    "         except Exception:\n",
    "              cache_name_to_print = \"objeto cached_content_resource existe pero sin .name\"\n",
    "    print(f\"Nombre del recurso de caché intentado usar: {cache_name_to_print}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9be3d6-b927-48fe-97ac-f368dbfa5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI con el modelo 'gemini-2.5-pro-preview-05-06' y contenido más largo...\n",
      "\n",
      "--- ¡Recurso Context Cache creado exitosamente! ---\n",
      "Nombre del recurso: 6811827477304836096\n",
      "Fecha de creación: 2025-05-16 19:59:48.893480+00:00\n",
      "Fecha de expiración: 2025-05-16 20:59:48.878533+00:00\n",
      "Modelo asociado: projects/zenda-adk/locations/us-central1/publishers/google/models/gemini-2.5-pro-preview-05-06\n",
      "Nota: No se pudo obtener el conteo de tokens directamente del objeto cached_content_resource.\n",
      "Nota: No se pudo obtener el tamaño en bytes directamente del objeto cached_content_resource.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 2.5 que queremos usar para el caché\n",
    "# Usamos el nombre que pareció reconocido en el intento anterior\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "\n",
    "# Contenido para cachear - Fragmento MUCHO más extenso del documento Zenda 7\n",
    "# Este texto está diseñado para superar HOLGADAMENTE los 1024 tokens mínimos requeridos.\n",
    "# Incluye partes de la introducción, arquitectura, gestión de contexto, manejo de pautas, temas, entidades y ajustes del MVP.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Zenda es un servicio de asistencia conversacional impulsado por inteligencia artificial (IA), orientado a personas que enfrentan desafíos personales, laborales o de desarrollo. El servicio ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas) de acompañamiento, reflexión, observación emocional y evaluación del progreso. Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes. El sistema opera de manera autónoma, basado en un conjunto estructurado de pautas de intervención generales (con planes de incorporar pautas específicas post-MVP) y con mecanismos de calidad (Think Tool interno, QA post-sesión), seguridad y trazabilidad avanzados. Busca ofrecer una experiencia conversacional cálida, empática, útil y profesional.\n",
    "\n",
    "La arquitectura general (MVP) de Zenda se compone de un Backend Principal desplegado en Google Cloud Run, utilizando contenedores Docker y Python con el Google Agent Development Kit (ADK). La Base de Datos es Supabase (PostgreSQL), alojando toda la información persistente como clientes, entidades, pautas, bitácora, sesiones, temas y especialidades. Los Agentes de IA son orquestados por ADK. Para el MVP, la secuencia principal involucra al Agente DT (Director Técnico - LlmAgent), el Agente Zenda (Agente Conversacional Principal - LlmAgent con Think Tool interno) y el Agente QA (Agente de Calidad - Lógica post-sesión, LlmAgent avanzado). La Interfaz de Usuario (MVP) es una aplicación en Streamlit para interacción vía chat de texto y voz. Se integran Servicios de Voz con Gemini Speech-to-Text (STT) y Text-to-Speech (TTS). Para la gestión de contexto y estado, se utiliza ADK State (Session Scope) para datos dinámicos y Vertex AI Context Caching para datos estables/grandes (Pautas, Especialidades, Resumen Histórico), aunque la implementación explícita de este último fue ajustada para el MVP según la Sección 15.\n",
    "\n",
    "La Gestión de Contexto, Estado y Memoria (MVP) es crucial para el desempeño de Zenda. El ADK State (Session Scope) almacena contenido como Entidades Activas (personas, orgs, jargon, conceptos), Criterios Pautas (actualizados por DT cada turno), Tema Elegido, Especialidad Principal/Secundarias, Preferencias Usuario (leídas de clientes) e Información del turno actual (emoción detectada, flags de riesgo básicos). Se usa como memoria de trabajo rápida para la lógica de los agentes (DT, Zenda) durante la sesión, actualizándose dinámicamente. El Vertex AI Context Caching está diseñado para contener Contenido como Pautas Generales (completas, desde Supabase), Definiciones de Especialidades (~35, desde Supabase) y el Resumen Histórico Acumulativo (última versión pre-calculada, desde Supabase sesiones.historical_summary). Su uso es para contexto más estático y/o voluminoso cargado al inicio de sesión, consumido principalmente por el LLM de Zenda para proporcionar conocimiento de fondo y definiciones sin sobrecargar el prompt directo, gestionado con TTLs apropiados. La Estrategia de Memoria Larga se adopta mediante el enfoque de resumen incremental/acumulativo generado post-sesión. Al final de la sesión N, el proceso de QA Post-Sesión invoca un LLM económico para leer la bitácora de la sesión N y el resumen acumulativo de N-1, generando un nuevo resumen acumulativo N que integra ambos. Este nuevo resumen se guarda en un campo dedicado (ej., historical_summary de tipo TEXT o JSONB) en la tabla `sesiones` asociada a la sesión N. Al inicio de la sesión N+1, DT (vía retrieve_summary_tool) lee este campo de la sesión N y lo carga en Context Caching (en el plan original antes de los ajustes del MVP).\n",
    "\n",
    "El Manejo de Pautas (MVP) es fundamental. El MVP operará con el conjunto existente de ~166 pautas generales transversales (archivo Pautas.csv). Estas pautas deben ser validadas por expertos antes de usarse en el MVP. Se implementará el Mecanismo de Adherencia Obligatoria: lógica explícita en Zenda para identificar la pauta general más relevante, extraer campos clave (Accion, Como, Para) y generar un borrador de respuesta intentando cumplir estrictamente con el Como para lograr el Para. También se preparará la declaración de la pauta usada para QA. Las Pautas Pendientes incluyen desarrollar e implementar nuevas pautas específicas para guiar la fase inicial de cada sesión (Acuerdo de Sesión). Las pautas generales validadas se almacenarán en la tabla `pautas` de Supabase.\n",
    "\n",
    "El Manejo de Temas y Especialidades (MVP) implica Definiciones: Tema (problema/objetivo del cliente) y Especialidad (enfoque profesional Zenda, ~35 tipos). El flujo de inicio de sesión implica que Cliente expone, Zenda/DT ayudan a elegir Tema, DT consulta tabla temas (vía Tool) para obtener Especialidad(es) recomendadas, y DT actualiza State. Zenda usa Especialidad(es) del State para adaptar su prompt/estilo/persona base y ayudar a contextualizar la selección de pautas generales.\n",
    "\n",
    "El Manejo de Entidades y Contexto Específico (MVP) usa la tabla `entidades`. Su propósito MVP es almacenar actores relacionales (personas, orgs) Y contexto específico del cliente (Jerga, Conceptos, Proyectos Mencionados, etc.), no para Progreso/Objetivos en MVP. La estructura final (MVP) se basa en la imagen image_d0421e.png. Se usa tipo_entidad Extendido añadiendo valores como \"Jargon\", \"Concepto\", \"ProyectoCliente\". La Coordinación entidades_tool <-> ADK State implica que DT carga entidades activas iniciales en State, Zenda lee primero del State, entidades_tool maneja lectura/escritura en Supabase, y tras una escritura exitosa, Zenda actualiza la entidad correspondiente en el State. La Búsqueda Contextual Interna LLM (Optimizada) implica que Zenda usa su conocimiento interno LLM \"on demand\" para definir/explicar términos/conceptos, guardando (vía tools) SOLO información muy específica del cliente o muy recurrente como entidad de tipo \"Jargon\" o \"Concepto\".\n",
    "\n",
    "Finalmente, la sección Ajustes al Alcance del MVP (Sección 15) detalla simplificaciones. En lugar de muchos agentes complejos, el MVP usa dos principales (DT y Zenda) con flujo secuencial fijo. Se eliminó el uso explícito del servicio Vertex AI Context Caching del alcance del MVP, basando el contexto de sesión en el ADK State y en incluir contexto relevante directamente en el prompt de ZendaAgentMVP (Implicit Caching). La persistencia de datos de sesión se simplificó a escrituras periódicas a una tabla intermedia y una escritura síncrona completa al final, difiriendo la pipeline asíncrona completa (Pub/Sub + Worker). Funcionalidades avanzadas como análisis emocional multimodal y detección de riesgos sofisticada se difieren, manteniendo solo interacción por voz STT/TTS y análisis emocional basado en texto. El Think Tool como callback con segundo LLM se difiere, basando la adherencia a pautas en el prompt de Zenda y la validación post-sesión del Agente QA, con DT alertando internamente. La implementación de la base de datos se enfocó solo en las tablas y campos indispensables para el MVP. El manejo de la tabla de entidades se simplificó a tipos básicos (Persona, Organizacion), difiriendo tipos extendidos como \"Jargon\" y relaciones detalladas. El mapeo Tema-Especialidad usa datos estáticos simples en lugar de consulta dinámica a tablas de Supabase. El front-end se enfoca en el núcleo conversacional. El registro de eventos en la bitácora se limita a eventos clave de alto nivel. El soporte multi-lenguaje es básico a nivel de infraestructura. El monitoreo se enfoca en logging estructurado de eventos clave. El proceso de despliegue es manual. El manejo de errores se enfoca en registro y robustez básica. Se mantiene el uso estratégico de Pydantic. El front-end en Streamlit es la prioridad 1 para el MVP.\n",
    "\"\"\" # Este texto combinado debería superar los 1024 tokens.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(f\"Intentando crear el recurso Context Cache en Vertex AI con el modelo '{MODEL_NAME}' y contenido más largo...\")\n",
    "    # La creación retorna el objeto del recurso cacheado\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    # Aquí intentamos imprimir los atributos, que causaron el error anterior, pero la creación SÍ funcionó\n",
    "    try:\n",
    "        print(f\"Tamaño del contenido cacheado (tokens): {cached_content_resource.token_count}\")\n",
    "    except AttributeError:\n",
    "         print(\"Nota: No se pudo obtener el conteo de tokens directamente del objeto cached_content_resource.\")\n",
    "    try:\n",
    "        print(f\"Tamaño del contenido cacheado (bytes): {cached_content_resource.size_bytes}\")\n",
    "    except AttributeError:\n",
    "         print(\"Nota: No se pudo obtener el tamaño en bytes directamente del objeto cached_content_resource.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado.\")\n",
    "    print(\"- Problemas de conexión o configuración de red.\")\n",
    "    print(\"- El contenido es menor a 1024 tokens.\")\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")\n",
    "    print(f\"El contenido tenía {len(contenido_para_cachear.split())} palabras (estimado de tokens).\") # Estimación simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c60a18b-fe7b-45b1-b366-77e2bf16286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Vertex AI para el proyecto 'zenda-adk' en la región 'us-central1'...\n",
      "Vertex AI inicializado correctamente.\n",
      "Intentando crear el recurso Context Cache en Vertex AI con el modelo 'gemini-2.5-pro-preview-05-06' y contenido más largo...\n",
      "\n",
      "--- ¡Recurso Context Cache creado exitosamente! ---\n",
      "Nombre del recurso: 6036082441490268160\n",
      "Fecha de creación: 2025-05-16 20:01:21.061531+00:00\n",
      "Fecha de expiración: 2025-05-16 21:01:21.046654+00:00\n",
      "Modelo asociado: projects/zenda-adk/locations/us-central1/publishers/google/models/gemini-2.5-pro-preview-05-06\n",
      "Nota: No se pudo obtener el conteo de tokens directamente del objeto cached_content_resource.\n",
      "Nota: No se pudo obtener el tamaño en bytes directamente del objeto cached_content_resource.\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import caching\n",
    "from vertexai.generative_models import Part\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# --- Configuración ---\n",
    "# El ID de tu proyecto de Google Cloud\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "# La ubicación requerida para Context Caching\n",
    "LOCATION = \"us-central1\"\n",
    "# El nombre del modelo Gemini 2.5 que queremos usar para el caché\n",
    "# Usamos el nombre que pareció reconocido en el intento anterior\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
    "\n",
    "\n",
    "# Contenido para cachear - Fragmento MUCHO más extenso del documento Zenda 7\n",
    "# Este texto está diseñado para superar HOLGADAMENTE los 1024 tokens mínimos requeridos.\n",
    "# Incluye partes de la introducción, arquitectura, gestión de contexto, manejo de pautas, temas, entidades y ajustes del MVP.\n",
    "contenido_para_cachear = \"\"\"\n",
    "Zenda es un servicio de asistencia conversacional impulsado por inteligencia artificial (IA), orientado a personas que enfrentan desafíos personales, laborales o de desarrollo. El servicio ofrece sesiones estructuradas con un agente IA avanzado (Zenda) capaz de adaptar su enfoque simulando múltiples especialidades profesionales, guiado por protocolos claros (pautas) de acompañamiento, reflexión, observación emocional y evaluación del progreso. Es una mezcla de coaching, desarrollo personal y asesoramiento, operado 100% por agentes inteligentes. El sistema opera de manera autónoma, basado en un conjunto estructurado de pautas de intervención generales (con planes de incorporar pautas específicas post-MVP) y con mecanismos de calidad (Think Tool interno, QA post-sesión), seguridad y trazabilidad avanzados. Busca ofrecer una experiencia conversacional cálida, empática, útil y profesional.\n",
    "\n",
    "La arquitectura general (MVP) de Zenda se compone de un Backend Principal desplegado en Google Cloud Run, utilizando contenedores Docker y Python con el Google Agent Development Kit (ADK). La Base de Datos es Supabase (PostgreSQL), alojando toda la información persistente como clientes, entidades, pautas, bitácora, sesiones, temas y especialidades. Los Agentes de IA son orquestados por ADK. Para el MVP, la secuencia principal involucra al Agente DT (Director Técnico - LlmAgent), el Agente Zenda (Agente Conversacional Principal - LlmAgent con Think Tool interno) y el Agente QA (Agente de Calidad - Lógica post-sesión, LlmAgent avanzado). La Interfaz de Usuario (MVP) es una aplicación en Streamlit para interacción vía chat de texto y voz. Se integran Servicios de Voz con Gemini Speech-to-Text (STT) y Text-to-Speech (TTS). Para la gestión de contexto y estado, se utiliza ADK State (Session Scope) para datos dinámicos y Vertex AI Context Caching para datos estables/grandes (Pautas, Especialidades, Resumen Histórico), aunque la implementación explícita de este último fue ajustada para el MVP según la Sección 15.\n",
    "\n",
    "La Gestión de Contexto, Estado y Memoria (MVP) es crucial para el desempeño de Zenda. El ADK State (Session Scope) almacena contenido como Entidades Activas (personas, orgs, jargon, conceptos), Criterios Pautas (actualizados por DT cada turno), Tema Elegido, Especialidad Principal/Secundarias, Preferencias Usuario (leídas de clientes) e Información del turno actual (emoción detectada, flags de riesgo básicos). Se usa como memoria de trabajo rápida para la lógica de los agentes (DT, Zenda) durante la sesión, actualizándose dinámicamente. El Vertex AI Context Caching está diseñado para contener Contenido como Pautas Generales (completas, desde Supabase), Definiciones de Especialidades (~35, desde Supabase) y el Resumen Histórico Acumulativo (última versión pre-calculada, desde Supabase sesiones.historical_summary). Su uso es para contexto más estático y/o voluminoso cargado al inicio de sesión, consumido principalmente por el LLM de Zenda para proporcionar conocimiento de fondo y definiciones sin sobrecargar el prompt directo, gestionado con TTLs apropiados. La Estrategia de Memoria Larga se adopta mediante el enfoque de resumen incremental/acumulativo generado post-sesión. Al final de la sesión N, el proceso de QA Post-Sesión invoca un LLM económico para leer la bitácora de la sesión N y el resumen acumulativo de N-1, generando un nuevo resumen acumulativo N que integra ambos. Este nuevo resumen se guarda en un campo dedicado (ej., historical_summary de tipo TEXT o JSONB) en la tabla `sesiones` asociada a la sesión N. Al inicio de la sesión N+1, DT (vía retrieve_summary_tool) lee este campo de la sesión N y lo carga en Context Caching (en el plan original antes de los ajustes del MVP).\n",
    "\n",
    "El Manejo de Pautas (MVP) es fundamental. El MVP operará con el conjunto existente de ~166 pautas generales transversales (archivo Pautas.csv). Estas pautas deben ser validadas por expertos antes de usarse en el MVP. Se implementará el Mecanismo de Adherencia Obligatoria: lógica explícita en Zenda para identificar la pauta general más relevante, extraer campos clave (Accion, Como, Para) y generar un borrador de respuesta intentando cumplir estrictamente con el Como para lograr el Para. También se preparará la declaración de la pauta usada para QA. Las Pautas Pendientes incluyen desarrollar e implementar nuevas pautas específicas para guiar la fase inicial de cada sesión (Acuerdo de Sesión). Las pautas generales validadas se almacenarán en la tabla `pautas` de Supabase.\n",
    "\n",
    "El Manejo de Temas y Especialidades (MVP) implica Definiciones: Tema (problema/objetivo del cliente) y Especialidad (enfoque profesional Zenda, ~35 tipos). El flujo de inicio de sesión implica que Cliente expone, Zenda/DT ayudan a elegir Tema, DT consulta tabla temas (vía Tool) para obtener Especialidad(es) recomendadas, y DT actualiza State. Zenda usa Especialidad(es) del State para adaptar su prompt/estilo/persona base y ayudar a contextualizar la selección de pautas generales.\n",
    "\n",
    "El Manejo de Entidades y Contexto Específico (MVP) usa la tabla `entidades`. Su propósito MVP es almacenar actores relacionales (personas, orgs) Y contexto específico del cliente (Jerga, Conceptos, Proyectos Mencionados, etc.), no para Progreso/Objetivos en MVP. La estructura final (MVP) se basa en la imagen image_d0421e.png. Se usa tipo_entidad Extendido añadiendo valores como \"Jargon\", \"Concepto\", \"ProyectoCliente\". La Coordinación entidades_tool <-> ADK State implica que DT carga entidades activas iniciales en State, Zenda lee primero del State, entidades_tool maneja lectura/escritura en Supabase, y tras una escritura exitosa, Zenda actualiza la entidad correspondiente en el State. La Búsqueda Contextual Interna LLM (Optimizada) implica que Zenda usa su conocimiento interno LLM \"on demand\" para definir/explicar términos/conceptos, guardando (vía tools) SOLO información muy específica del cliente o muy recurrente como entidad de tipo \"Jargon\" o \"Concepto\".\n",
    "\n",
    "Finalmente, la sección Ajustes al Alcance del MVP (Sección 15) detalla simplificaciones. En lugar de muchos agentes complejos, el MVP usa dos principales (DT y Zenda) con flujo secuencial fijo. Se eliminó el uso explícito del servicio Vertex AI Context Caching del alcance del MVP, basando el contexto de sesión en el ADK State y en incluir contexto relevante directamente en el prompt de ZendaAgentMVP (Implicit Caching). La persistencia de datos de sesión se simplificó a escrituras periódicas a una tabla intermedia y una escritura síncrona completa al final, difiriendo la pipeline asíncrona completa (Pub/Sub + Worker). Funcionalidades avanzadas como análisis emocional multimodal y detección de riesgos sofisticada se difieren, manteniendo solo interacción por voz STT/TTS y análisis emocional basado en texto. El Think Tool como callback con segundo LLM se difiere, basando la adherencia a pautas en el prompt de Zenda y la validación post-sesión del Agente QA, con DT alertando internamente. La implementación de la base de datos se enfocó solo en las tablas y campos indispensables para el MVP. El manejo de la tabla de entidades se simplificó a tipos básicos (Persona, Organizacion), difiriendo tipos extendidos como \"Jargon\" y relaciones detalladas. El mapeo Tema-Especialidad usa datos estáticos simples en lugar de consulta dinámica a tablas de Supabase. El front-end se enfoca en el núcleo conversacional. El registro de eventos en la bitácora se limita a eventos clave de alto nivel. El soporte multi-lenguaje es básico a nivel de infraestructura. El monitoreo se enfoca en logging estructurado de eventos clave. El proceso de despliegue es manual. El manejo de errores se enfoca en registro y robustez básica. Se mantiene el uso estratégico de Pydantic. El front-end en Streamlit es la prioridad 1 para el MVP.\n",
    "\"\"\" # Este texto combinado debería superar los 1024 tokens.\n",
    "\n",
    "# --- Inicializar Vertex AI ---\n",
    "# Esto le dice a la librería con qué proyecto y en qué región trabajar\n",
    "try:\n",
    "    print(f\"Inicializando Vertex AI para el proyecto '{PROJECT_ID}' en la región '{LOCATION}'...\")\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(\"Vertex AI inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar Vertex AI: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que el Project ID sea correcto.\")\n",
    "    # Si falla la inicialización, detenemos el proceso aquí\n",
    "    # En Jupyter, puedes continuar en otra celda si corriges el problema\n",
    "    raise # Relanzamos la excepción para que veas el error completo si ocurre\n",
    "\n",
    "\n",
    "# --- Crear el Context Cache ---\n",
    "try:\n",
    "    print(f\"Intentando crear el recurso Context Cache en Vertex AI con el modelo '{MODEL_NAME}' y contenido más largo...\")\n",
    "    # La creación retorna el objeto del recurso cacheado\n",
    "    # ¡Este objeto es lo que necesitamos para la variable cached_content_resource!\n",
    "    cached_content_resource = caching.CachedContent.create(\n",
    "        model_name=MODEL_NAME,\n",
    "        contents=[Part.from_text(contenido_para_cachear)],\n",
    "        ttl=datetime.timedelta(hours=1), # El caché durará 1 hora (TTL = Time To Live)\n",
    "        display_name=\"zenda-ejemplo-pauta-cache\" # Un nombre amigable para identificarlo\n",
    "    )\n",
    "    print(\"\\n--- ¡Recurso Context Cache creado exitosamente! ---\")\n",
    "    print(f\"Nombre del recurso: {cached_content_resource.name}\")\n",
    "    print(f\"Fecha de creación: {cached_content_resource.create_time}\")\n",
    "    print(f\"Fecha de expiración: {cached_content_resource.expire_time}\")\n",
    "    print(f\"Modelo asociado: {cached_content_resource.model_name}\")\n",
    "    # Aquí intentamos imprimir los atributos, que causaron el error anterior, pero la creación SÍ funcionó\n",
    "    try:\n",
    "        print(f\"Tamaño del contenido cacheado (tokens): {cached_content_resource.token_count}\")\n",
    "    except AttributeError:\n",
    "         print(\"Nota: No se pudo obtener el conteo de tokens directamente del objeto cached_content_resource.\")\n",
    "    try:\n",
    "        print(f\"Tamaño del contenido cacheado (bytes): {cached_content_resource.size_bytes}\")\n",
    "    except AttributeError:\n",
    "         print(\"Nota: No se pudo obtener el tamaño en bytes directamente del objeto cached_content_resource.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error al crear el Context Cache ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"- Permisos insuficientes para crear recursos de caching en tu proyecto.\")\n",
    "    print(\"- Error en el nombre del modelo especificado.\")\n",
    "    print(\"- Problemas de conexión o configuración de red.\")\n",
    "    print(\"- El contenido es menor a 1024 tokens.\")\n",
    "    print(\"Si estás en Workbench, verifica que tienes permisos para Vertex AI.\")\n",
    "    print(f\"El contenido tenía {len(contenido_para_cachear.split())} palabras (estimado de tokens).\") # Estimación simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fe0079-7dfb-4099-9a0d-73f56db4d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recurso de caché '6036082441490268160' encontrado. Procediendo con el Paso 3...\n",
      "\n",
      "Inicializando cliente google.genai para proyecto 'zenda-adk' en región 'us-central1'...\n",
      "Cliente google.genai inicializado correctamente.\n",
      "Intentando usar el caché '6036082441490268160' en una llamada al modelo 'gemini-2.5-pro-preview-05-06' usando cliente google.genai...\n",
      "Llamando al modelo generate_content con caché...\n",
      "\n",
      "--- Respuesta del modelo usando el Context Cache (cliente google.genai) ---\n",
      "Basándome exclusivamente en la información proporcionada:\n",
      "\n",
      "1.  **¿Qué es Zenda?**\n",
      "    Zenda es un servicio de asistencia conversacional impulsado por IA, orientado a personas con desafíos personales, laborales o de desarrollo. Ofrece sesiones estructuradas con un agente IA que simula múltiples especialidades profesionales, guiado por pautas, combinando coaching, desarrollo personal y asesoramiento.\n",
      "\n",
      "2.  **¿Cuáles son los 3 agentes principales que lo componen?**\n",
      "    Los 3 agentes principales en el MVP son:\n",
      "    *   Agente DT (Director Técnico)\n",
      "    *   Agente Zenda (Agente Conversacional Principal)\n",
      "    *   Agente QA (Agente de Calidad, con lógica post-sesión)\n",
      "\n",
      "3.  **¿Qué mecanismos de gestión de contexto y memoria se mencionan?**\n",
      "    Se mencionan:\n",
      "    *   **ADK State (Session Scope):** Para memoria de trabajo rápida y datos dinámicos durante la sesión.\n",
      "    *   **Vertex AI Context Caching:** Originalmente para datos estables/grandes como pautas y resúmenes históricos (aunque su uso explícito se eliminó en el MVP, optando por ADK State e \"Implicit Caching\" vía prompt).\n",
      "    *   **Estrategia de Memoria Larga:** Mediante un resumen incremental/acumulativo generado post-sesión.\n",
      "\n",
      "--- Intentando mostrar metadatos de uso (cliente google.genai) ---\n",
      "Metadatos de uso encontrados:\n",
      "- cache_tokens_details: None\n",
      "- cached_content_token_count: 1774\n",
      "- candidates_token_count: 287\n",
      "- candidates_tokens_details: [ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)]\n",
      "- construct: <bound method BaseModel.construct of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- copy: <bound method BaseModel.copy of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- dict: <bound method BaseModel.dict of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- from_orm: <bound method BaseModel.from_orm of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- json: <bound method BaseModel.json of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- model_computed_fields: {}\n",
      "- model_config: {'alias_generator': <function to_camel at 0x7fea23cdc160>, 'populate_by_name': True, 'from_attributes': True, 'protected_namespaces': (), 'extra': 'forbid', 'arbitrary_types_allowed': True, 'ser_json_bytes': 'base64', 'val_json_bytes': 'base64', 'ignored_types': (<class 'typing.TypeVar'>,), 'validate_by_alias': True, 'validate_by_name': True}\n",
      "- model_construct: <bound method BaseModel.model_construct of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_copy: <bound method BaseModel.model_copy of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- model_dump: <bound method BaseModel.model_dump of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- model_dump_json: <bound method BaseModel.model_dump_json of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- model_extra: None\n",
      "- model_fields: {'cache_tokens_details': FieldInfo(annotation=Union[list[ModalityTokenCount], NoneType], required=False, default=None, alias='cacheTokensDetails', alias_priority=1, description='Output only. List of modalities of the cached content in the request input.'), 'cached_content_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='cachedContentTokenCount', alias_priority=1, description='Output only. Number of tokens in the cached part in the input (the cached content).'), 'candidates_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='candidatesTokenCount', alias_priority=1, description='Number of tokens in the response(s).'), 'candidates_tokens_details': FieldInfo(annotation=Union[list[ModalityTokenCount], NoneType], required=False, default=None, alias='candidatesTokensDetails', alias_priority=1, description='Output only. List of modalities that were returned in the response.'), 'prompt_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='promptTokenCount', alias_priority=1, description='Number of tokens in the request. When `cached_content` is set, this is still the total effective prompt size meaning this includes the number of tokens in the cached content.'), 'prompt_tokens_details': FieldInfo(annotation=Union[list[ModalityTokenCount], NoneType], required=False, default=None, alias='promptTokensDetails', alias_priority=1, description='Output only. List of modalities that were processed in the request input.'), 'thoughts_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='thoughtsTokenCount', alias_priority=1, description='Output only. Number of tokens present in thoughts output.'), 'tool_use_prompt_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='toolUsePromptTokenCount', alias_priority=1, description='Output only. Number of tokens present in tool-use prompt(s).'), 'tool_use_prompt_tokens_details': FieldInfo(annotation=Union[list[ModalityTokenCount], NoneType], required=False, default=None, alias='toolUsePromptTokensDetails', alias_priority=1, description='Output only. List of modalities that were processed for tool-use request inputs.'), 'total_token_count': FieldInfo(annotation=Union[int, NoneType], required=False, default=None, alias='totalTokenCount', alias_priority=1, description='Total token count for prompt, response candidates, and tool-use prompts (if present).'), 'traffic_type': FieldInfo(annotation=Union[TrafficType, NoneType], required=False, default=None, alias='trafficType', alias_priority=1, description='Output only. Traffic type. This shows whether a request consumes Pay-As-You-Go or Provisioned Throughput quota.')}\n",
      "- model_fields_set: {'prompt_tokens_details', 'thoughts_token_count', 'total_token_count', 'traffic_type', 'candidates_token_count', 'prompt_token_count', 'cached_content_token_count', 'candidates_tokens_details'}\n",
      "- model_json_schema: <bound method BaseModel.model_json_schema of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_parametrized_name: <bound method BaseModel.model_parametrized_name of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_post_init: <bound method BaseModel.model_post_init of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- model_rebuild: <bound method BaseModel.model_rebuild of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_validate: <bound method BaseModel.model_validate of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_validate_json: <bound method BaseModel.model_validate_json of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- model_validate_strings: <bound method BaseModel.model_validate_strings of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- parse_file: <bound method BaseModel.parse_file of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- parse_obj: <bound method BaseModel.parse_obj of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- parse_raw: <bound method BaseModel.parse_raw of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- prompt_token_count: 1855\n",
      "- prompt_tokens_details: [ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)]\n",
      "- schema: <bound method BaseModel.schema of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- schema_json: <bound method BaseModel.schema_json of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- thoughts_token_count: 1963\n",
      "- to_json_dict: <bound method BaseModel.to_json_dict of GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=1774, candidates_token_count=287, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=287)], prompt_token_count=1855, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1855)], thoughts_token_count=1963, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=4105, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>)>\n",
      "- tool_use_prompt_token_count: None\n",
      "- tool_use_prompt_tokens_details: None\n",
      "- total_token_count: 4105\n",
      "- traffic_type: ON_DEMAND\n",
      "- update_forward_refs: <bound method BaseModel.update_forward_refs of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n",
      "- validate: <bound method BaseModel.validate of <class 'google.genai.types.GenerateContentResponseUsageMetadata'>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_5450/2380921416.py:132: PydanticDeprecatedSince211: Accessing this attribute on the instance is deprecated, and will be removed in Pydantic V3. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  value = getattr(response.usage_metadata, attr)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Importamos google.genai y types\n",
    "import google.genai\n",
    "from google.genai import types\n",
    "# Importamos vertexai también, ya que lo usaremos en la inicialización del cliente genai\n",
    "import vertexai\n",
    "\n",
    "# --- Configuración ---\n",
    "# Estas variables deberían estar definidas (de la celda anterior o definidas aquí si corres solo esta celda)\n",
    "PROJECT_ID = 'zenda-adk'\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-pro-preview-05-06\" # El modelo usado para crear el caché\n",
    "\n",
    "# --- Asegurar que el recurso de caché esté disponible (Mandatorio) ---\n",
    "# La variable 'cached_content_resource' debe estar definida en esta sesión de Jupyter.\n",
    "\n",
    "# --- Instrucción: Ejecuta la celda del Paso 2 PRIMERO o usa la OPCIÓN 2 ---\n",
    "# Si acabas de ejecutar la celda del \"Paso 2\" con éxito en esta misma sesión, la variable existe.\n",
    "# Si reiniciaste el kernel, necesitas recuperarla.\n",
    "\n",
    "# OPCIÓN 2 (Recuperar si reiniciaste el kernel): Descomenta y ejecuta este bloque\n",
    "# ANTES de la llamada al modelo si la variable no está definida.\n",
    "# try:\n",
    "#     print(\"OPCIÓN 2: Intentando recuperar el recurso de caché por nombre...\")\n",
    "#     # Reemplaza 'EL_ID_NUMERICO_DE_TU_CACHE' con el número ID que obtuviste en la salida exitosa del Paso 2.\n",
    "#     # ¡TU ÚLTIMO CACHE ID EXITOSO ES: 6036082441490268160 ! Confirma que este número es correcto para ti.\n",
    "#     cache_resource_id = '6036082441490268160' # <<< ¡USA ESTE ÚLTIMO ID!\n",
    "#\n",
    "#     cache_resource_name_str = f\"projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents/{cache_resource_id}\"\n",
    "#     genai_client_for_retrieval = google.genai.Client() # Cliente simple para get_cached_content\n",
    "#     cached_content_resource = genai_client_for_retrieval.get_cached_content(name=cache_resource_name_str)\n",
    "#     print(f\"Recurso de caché '{cache_resource_name_str}' recuperado exitosamente.\")\n",
    "#     print(\"La variable 'cached_content_resource' ahora debería estar disponible en tu sesión.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"--- Error al recuperar el recurso de caché por nombre (OPCIÓN 2) ---\")\n",
    "#     print(f\"Mensaje de error: {e}\")\n",
    "#     cached_content_resource = None # Aseguramos que quede None si hay error\n",
    "#     # No detenemos la ejecución aquí, el check de abajo manejará si no se recuperó\n",
    "\n",
    "# --- VERIFICACIÓN MANDATORIA ---\n",
    "# Verificamos si la variable existe después de intentar la OPCIÓN 1 (correr Paso 2) o la OPCIÓN 2.\n",
    "if 'cached_content_resource' not in locals() or cached_content_resource is None:\n",
    "     print(\"\\n--- ERROR FATAL: La variable 'cached_content_resource' NO está disponible. ---\")\n",
    "     print(\"Debes ejecutar PRIMERO la celda del 'Paso 2' (Mandatorio) CON ÉXITO\")\n",
    "     print(\"o usar la OPCIÓN 2 de recuperación de caché (descomentando y ejecutando ese bloque) si reiniciaste el kernel.\")\n",
    "     # No podemos continuar sin el recurso. Salimos.\n",
    "     exit() # Detiene la ejecución si el recurso no está listo\n",
    "else:\n",
    "     # Si la variable existe, mostramos el nombre del recurso que vamos a usar para confirmación\n",
    "     print(f\"\\nRecurso de caché '{cached_content_resource.name}' encontrado. Procediendo con el Paso 3...\")\n",
    "\n",
    "\n",
    "# --- Inicializar Cliente google.genai (¡CORREGIDO!) ---\n",
    "# Usaremos google.genai.Client y le pasamos explícitamente la configuración de Vertex AI\n",
    "try:\n",
    "    print(f\"\\nInicializando cliente google.genai para proyecto '{PROJECT_ID}' en región '{LOCATION}'...\")\n",
    "    # ¡CORRECCIÓN FINAL! Pasamos el módulo vertexai, el proyecto y la ubicación al inicializador del cliente genai\n",
    "    # Esto le dice al cliente genai que use el backend de Vertex AI.\n",
    "    genai_client = google.genai.Client(\n",
    "        vertexai=vertexai, # Referencia al módulo vertexai inicializado\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION\n",
    "        # También se puede intentar con client_options={'api_endpoint': f\"{LOCATION}-aiplatform.googleapis.com\"}\n",
    "        # si la forma anterior no funciona, pero esta es más directa según la doc.\n",
    "    )\n",
    "    print(\"Cliente google.genai inicializado correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- Error al inicializar cliente google.genai ---\")\n",
    "    print(f\"Mensaje de error: {e}\")\n",
    "    print(\"Asegúrate de que tu entorno esté autenticado con GCP y que tengas permisos de Vertex AI.\")\n",
    "    print(\"Si estás fuera de GCP, verifica GOOGLE_APPLICATION_CREDENTIALS o GOOGLE_API_KEY.\")\n",
    "    # No detenemos la ejecución aquí si el cliente falla, el error en la llamada al modelo lo indicará\n",
    "    genai_client = None # Aseguramos que el cliente es None si falló\n",
    "\n",
    "\n",
    "# --- Usar el Context Cache en una llamada al modelo usando el cliente google.genai ---\n",
    "# Solo intentamos esto si el cliente se inicializó correctamente\n",
    "if genai_client:\n",
    "    try:\n",
    "        print(f\"Intentando usar el caché '{cached_content_resource.name}' en una llamada al modelo '{MODEL_NAME}' usando cliente google.genai...\")\n",
    "\n",
    "        # Define tu prompt. Este prompt se combinará con el contenido cacheado por el modelo.\n",
    "        prompt_usuario = \"\"\"\n",
    "        Basándote EXCLUSIVAMENTE en la información que te fue proporcionada como contexto (el contenido cacheado),\n",
    "        describe brevemente:\n",
    "        1. ¿Qué es Zenda?\n",
    "        2. ¿Cuáles son los 3 agentes principales que lo componen?\n",
    "        3. ¿Qué mecanismos de gestión de contexto y memoria se mencionan?\n",
    "        Responde de forma concisa.\n",
    "        \"\"\"\n",
    "\n",
    "        # Creamos un objeto GenerateContentConfig (usando types de google.genai)\n",
    "        # Y le pasamos el nombre completo del recurso de caché.\n",
    "        # Este objeto se pasará al parámetro 'config' de generate_content.\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            cached_content=cached_content_resource.name # Pasamos el nombre completo del recurso de caché aquí dentro\n",
    "            # Puedes añadir otros parámetros de generación si quieres (ej. temperatura, max_output_tokens)\n",
    "            # temperature=0.2, max_output_tokens=300 # Ejemplo\n",
    "        )\n",
    "\n",
    "        # Llama al modelo usando el cliente genai.models.generate_content.\n",
    "        # Para este cliente, el nombre del modelo se pasa con el path completo del recurso en Vertex AI.\n",
    "        full_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_NAME}\"\n",
    "\n",
    "        print(\"Llamando al modelo generate_content con caché...\")\n",
    "        response = genai_client.models.generate_content(\n",
    "            model=full_model_name, # Nombre completo del modelo como recurso de Vertex AI\n",
    "            contents=[{\"text\": prompt_usuario}], # El prompt como una lista de dicts con clave 'text'\n",
    "            config=generation_config # Pasamos el objeto de configuración AHORA sí al parámetro 'config'\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Respuesta del modelo usando el Context Cache (cliente google.genai) ---\")\n",
    "        # La respuesta del texto suele estar en response.candidates[0].content.parts[0].text\n",
    "        try:\n",
    "            response_text = response.candidates[0].content.parts[0].text\n",
    "            print(response_text)\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo extraer el texto de la respuesta fácilmente: {e}\")\n",
    "            print(\"Estructura completa de la respuesta para depurar:\")\n",
    "            print(response)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Intentando mostrar metadatos de uso (cliente google.genai) ---\")\n",
    "        if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "             print(\"Metadatos de uso encontrados:\")\n",
    "             # Imprimimos todos los atributos disponibles en usage_metadata\n",
    "             # El conteo de tokens del caché usado debería aparecer aquí si la llamada lo reporta\n",
    "             for attr in dir(response.usage_metadata):\n",
    "                 if not attr.startswith('_'): # Ignorar atributos internos de Python\n",
    "                     try:\n",
    "                         value = getattr(response.usage_metadata, attr)\n",
    "                         print(f\"- {attr}: {value}\")\n",
    "                     except Exception:\n",
    "                         print(f\"- {attr}: [No se pudo obtener el valor]\")\n",
    "\n",
    "        else:\n",
    "            print(\"No se encontraron metadatos de uso detallados en la respuesta.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Error al usar el Context Cache en la llamada al modelo (cliente google.genai) ---\")\n",
    "        print(f\"Mensaje de error: {e}\")\n",
    "        print(\"Posibles causas:\")\n",
    "        print(\"- El recurso de caché ya expiró o fue eliminado (dura 1 hora desde que lo creaste).\")\n",
    "        print(f\"- Problemas de conexión o configuración. Asegúrate de que estás en '{LOCATION}'.\")\n",
    "        print(\"- Error en el nombre del recurso de caché o del modelo.\")\n",
    "        print(\"- El prompt, el caché o la combinación causaron un error interno del modelo.\")\n",
    "        print(\"Asegúrate de que el cliente genai está inicializado y autenticado correctamente.\")\n",
    "        cache_name_to_print = \"variable cached_content_resource no definida\"\n",
    "        if 'cached_content_resource' in locals() and cached_content_resource:\n",
    "             try:\n",
    "                 cache_name_to_print = cached_content_resource.name\n",
    "             except Exception:\n",
    "                  cache_name_to_print = \"objeto cached_content_resource existe pero sin .name\"\n",
    "        print(f\"Nombre del recurso de caché intentado usar: {cache_name_to_print}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo se pudo intentar la llamada al modelo porque el cliente 'genai_client' no se inicializó correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87aed4c-b0e0-4f32-972f-faed2539d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
